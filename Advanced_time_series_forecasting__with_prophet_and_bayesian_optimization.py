# -*- coding: utf-8 -*-
"""Advanced_Time_Series_Forecasting _with_Prophet_and_Bayesian_Optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LkTjT3RnUH8XoT-Z9AVQhLWxosqzB0ox
"""

# ==========================================
# Advanced Time Series Forecasting with
# Prophet + Bayesian Optimization (Optuna)
# ==========================================

import numpy as np
import pandas as pd
import optuna
from prophet import Prophet
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# -----------------------------
# 1. Generate Synthetic Dataset
# -----------------------------
def generate_time_series():
    np.random.seed(42)

    dates = pd.date_range(start="2018-01-01", periods=1500, freq="D")

    # Trend
    trend = np.linspace(10, 50, len(dates))

    # Seasonality
    yearly = 15 * np.sin(2 * np.pi * dates.dayofyear / 365)
    weekly = 5 * np.sin(2 * np.pi * dates.dayofweek / 7)

    # Noise
    noise = np.random.normal(0, 2, len(dates))

    y = trend + yearly + weekly + noise

    df = pd.DataFrame({"ds": dates, "y": y})

    # Inject anomalies (spikes/dips)
    anomaly_idx = np.random.choice(len(df), 5, replace=False)
    df.loc[anomaly_idx, "y"] += np.random.choice([25, -25], size=5)

    return df


df = generate_time_series()

# -----------------------------
# 2. Train / Validation / Test Split
# -----------------------------
train_size = int(len(df) * 0.7)
val_size = int(len(df) * 0.15)

train_df = df[:train_size]
val_df = df[train_size:train_size + val_size]
test_df = df[train_size + val_size:]

# -----------------------------
# 3. Evaluation Metrics
# -----------------------------
def wape(y_true, y_pred):
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))


def evaluate(y_true, y_pred):
    return {
        "RMSE": np.sqrt(mean_squared_error(y_true, y_pred)),
        "MAE": mean_absolute_error(y_true, y_pred),
        "WAPE": wape(y_true, y_pred)
    }

# -----------------------------
# 4. Baseline Prophet Model
# -----------------------------
baseline_model = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False
)

baseline_model.fit(train_df)

future_val = baseline_model.make_future_dataframe(periods=len(val_df))
forecast_val = baseline_model.predict(future_val)

baseline_pred = forecast_val.iloc[-len(val_df):]["yhat"].values
baseline_metrics = evaluate(val_df["y"].values, baseline_pred)

print("\nBaseline Model Performance:")
print(baseline_metrics)

# -----------------------------
# 5. Bayesian Optimization (Optuna)
# -----------------------------
def objective(trial):
    params = {
        "seasonality_prior_scale": trial.suggest_float("seasonality_prior_scale", 1, 20),
        "changepoint_prior_scale": trial.suggest_float("changepoint_prior_scale", 0.001, 0.5),
        "holidays_prior_scale": trial.suggest_float("holidays_prior_scale", 1, 20)
    }

    model = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False,
        **params
    )

    model.fit(train_df)

    future = model.make_future_dataframe(periods=len(val_df))
    forecast = model.predict(future)

    preds = forecast.iloc[-len(val_df):]["yhat"].values
    rmse = np.sqrt(mean_squared_error(val_df["y"].values, preds))

    return rmse


study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=30)

print("\nBest Hyperparameters:")
print(study.best_params)

# -----------------------------
# 6. Final Optimized Model
# -----------------------------
best_model = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False,
    **study.best_params
)

best_model.fit(pd.concat([train_df, val_df]))

future_test = best_model.make_future_dataframe(periods=len(test_df))
forecast_test = best_model.predict(future_test)

optimized_pred = forecast_test.iloc[-len(test_df):]["yhat"].values
optimized_metrics = evaluate(test_df["y"].values, optimized_pred)

print("\nOptimized Model Performance:")
print(optimized_metrics)

# -----------------------------
# 7. Comparison Summary
# -----------------------------
comparison = pd.DataFrame({
    "Metric": ["RMSE", "MAE", "WAPE"],
    "Baseline": list(baseline_metrics.values()),
    "Optimized": list(optimized_metrics.values())
})

print("\nPerformance Comparison:")
print(comparison)

# -----------------------------
# 8. Plot Forecast
# -----------------------------
plt.figure(figsize=(12, 5))
plt.plot(test_df["ds"], test_df["y"], label="Actual", color="black")
plt.plot(test_df["ds"], optimized_pred, label="Optimized Forecast", color="red")
plt.legend()
plt.title("Optimized Prophet Forecast vs Actual")
plt.show()